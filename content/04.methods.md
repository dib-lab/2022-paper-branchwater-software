## Methods {.page_break_before}

*The Methods should include a subsection on Implementation describing
 how the tool works and any relevant technical details required for
 implementation; and a subsection on Operation, which should include
 the minimal system requirements needed to run the software and an
 overview of the workflow.*
 
### Wort and the SRA digest

We determined the accessions of all publicly available shotgun
metagenomic via the query string `"METAGENOMIC"[Source] NOT
amplicon[All Fields]` at the NCBI Sequence Read Archive Web site,
https://www.ncbi.nlm.nih.gov/sra.  We then downloaded all runs for all
accessions and streamed them into `sourmash sketch dna` with
parameters `-p k=21,31,51,scaled=1000,abund` and saved them as
individual gzipped JSON files for each input run.

The resulting metagenome data set sizes were XXX TB.
(Describe number, sum, average, median, mode of sketch file sizes.)

### Implementation of sra_search - rust; uses sourmash core.

The `sra_search` program implements the following steps:

1. Loads the query sketches into memory from a list of files.
2. Loads the list of filenames containing subject sketches to search.
3. In a Rust closure function executed in parallel for each subject sketch filename,
   a. loads the subject sketch from the file;
   b. for each query, determines the estimated overlap between query and subject;
   c. reports overlaps above a user-specified threshold.
   d. releases all per-metagenome resources

Note that any requested downsampling of sketches is performed
dynamically, after load. Results are reported back to a separate
"writer" thread via a threadsafe multi-producer, single-consumer FIFO
queue.

This approach leverages the core features of sourmash to efficiently
keep queries in memory and batch-process metagenome sketches without
storing them all in memory. The approach also takes advantage of
the effective immutability of queries, which can be easily shared
without data races by multiple processing threads.

### Executing sra_search at the command line

sra_search takes in search parameters as well as two text files, one
containing a list of query file paths and one containing a lits of
subject file paths. Upon execution, it reports the number of query
sketches loaded and the number of subject file paths found, and then
begins the search. It progressively reports the number of sketches
searched in blocks of 10000 along with any matches.

We typically run sra_search in a snakemake workflow, which manages
environment variables and input/output files.

### Performance - scales linearly in memory and time with queries and # threads

A variety of simple benchmarks will presumably show:

* Speed increases linearly with number of threads
* Memory scales linearly: sum(queries) + sum(threads\*average size of
  sketches)
* What happens w/biggest sketches?
* Complexity is n(query) \* n(subject), with subject loading being the
  dominant practical time. More complex indexing and query foo could
  be done but itâ€™s fast enough and the code is simple.
  
Do this on ~1000-10,000 sketches. Make repo, do benchmarks.

Sra_search is largely I/O bound. Presumably we could speed it up a bit
by distributing sketches to various nodes but in practice this is
logistically challenging to coordinate. 13 TB is large. In a cloud
environment with fast interconnect other design decisions could be
made. But also other query systems could be built.

### Post-search validation etc. Testing.

It is straightforward to use sourmash CLI to query the metagenome data
sets to double check magsearch results. This is usually only internal
technical validation since magsearch is built on the same code that
sourmash uses, but is a recommended first step because the sourmash UX
is better and the output is richer (e.g. weighted abundances,e tc.)

FracMinHash generally and MAGsearch specifically have been validated
in a more scientific sense primarily by mapping reads. This is
discussed further below.

### Sra_search is inexpensive and supports exploratory queries

Estimate cost of a run. Compare to serratus - cloud compute, data
download. Serratus is probably cheaper than $20k now but still
expensive.
