## Methods {.page_break_before}

<!-- XXX diagram -->

<!-- *The Methods should include a subsection on Implementation describing
 how the tool works and any relevant technical details required for
 implementation; and a subsection on Operation, which should include
 the minimal system requirements needed to run the software and an
 overview of the workflow.* -->

### Sketching the Sequence Read Archive

We determined the accessions of all publicly available shotgun
metagenomic via the query string `"METAGENOMIC"[Source] NOT
amplicon[All Fields]` at the NCBI Sequence Read Archive Web site,
https://www.ncbi.nlm.nih.gov/sra.  We then downloaded all runs for all
accessions and streamed them into `sourmash sketch dna` with
parameters `-p k=21,31,51,scaled=1000,abund`. The output sourmash
signature files were saved as individual gzipped JSON files (each
containing 3 sketches), one file for each input run.

The resulting catalog contains 767,277 metagenome data sets as of
March 2022, with the largest category annotated as human-associated microbiomes
(Table @tbl:sra-types). The size of all sketches together is 7.5 TB,
containing approximately 375 billion hashes per k-mer size,
representing 375 trillion k-mers <!-- from ZZZ PB of original files CTB -->. The
average sketch file size is 9.7 MB, and the median file size is 570kb. The
largest 10,000 data sets comprise 30% of the total sketch sizes.
<!-- NTP question: how big is largest sketch? XXX -->

| "Scientific Name" provided by submitter | distinct data sets |
| --- | --- |
| human gut metagenome  |   162187 |
| metagenome              |   57048
| gut metagenome           | 47244
| human metagenome         | 36438
| soil metagenome          | 35323
| mouse gut metagenome     | 26482
| human skin metagenome    | 25700
| Homo sapiens             | 21020
| marine metagenome        |14400
| human oral metagenome    |  14235

Table: The 10 largest categories of metagenome data set types in the Sequence
Read Archive, as of March 2022. {#tbl:sra-types}

### Implementation of multithreaded search

<!-- CTB: rename program to branchwater? -->

Branchwater program is built in Rust on top of the sourmash
library for loading and comparing sketches. It implements the
following steps:

1. Loads all query sketches into memory from a list of files.
2. Loads the list of filenames containing subject sketches to search.
3. In a Rust closure function executed in parallel for each subject sketch filename,
   a. loads the subject sketch from the file;
   b. for each query, determines the estimated overlap between query and subject;
   c. reports overlaps above a user-specified threshold.
   d. releases all per-metagenome resources

Downsampling of sketches to higher scaled values is performed
dynamically, after load (if requested). Results are reported back to a
separate "writer" thread via a threadsafe multi-producer,
single-consumer FIFO queue. We use the rayon `par_iter` function to
execute the closures in parallel.

This approach leverages the core features of sourmash to efficiently
keep queries in memory and batch-process metagenome sketches without
storing them all in memory. The approach also takes advantage of the
effective immutability of queries, which can be shared without
data races by multiple processing threads.

### Executing branchwater at the command line

branchwater takes in search parameters as well as two text files, one
containing a list of query file paths and one containing a list of
subject file paths. Upon execution, it reports the number of query
sketches loaded and the number of subject file paths found, and then
begins the search. It progressively reports the number of sketches
searched in blocks of 10000, and outputs matches to a CSV File.

We typically run branchwater in a snakemake workflow, which manages
environment variables and input/output files.

### Performance and scaling analysis

In Tables @tbl:avg_bench and @tbl:catalog we show performance metrics
for branchwater. Table @tbl:avg_bench contains average measurements
and standard deviations for time, memory, and I/O, showing that
branchwater is I/O and memory intensive. Table @tbl:catalog compares
the runtimes and memory usage for a search the entire catalog
(normalized to 10k samples) against both the random subsets in Table
@tbl_avg_bench and the largest 10k sketches in the database. The
increased memory usage of the catalog and especially the 10k largest
metagenomes suggests that a relatively small number of metagenomes
contributes the bulk of both time and memory usage to a full catalog
search of branchwater.

| metric | observed | 
| -------- | -------- | 
| time     | 24.2 +/- 1.7 min |
| max RSS | 16.4 +/-  1.6 GB |
| I/O in | 93.4 +/- 1.9 GB |

Table: Time, memory, and I/O input for 5 runs of 1000 queries against 10,000 metagenomes. Queries were randomly selected from 318k genomes in GTDB rs207. Metagenomes were randomly selected from the full catalog of 767k. {#tbl:avg_bench}

| database | time | max RSS |
| -------- | -------- | -------- |
| entire catalog | 27.2m$^*$ | 52.5 GB |
| random 10k subset | 24.2m   | 16.4 GB |
| 10k largest metagenomes | 594.0 m | 60.0 GB |

Table: A comparison of database searches for the entire catalog of
767k metagenomes, with time normalized to 10k, the average of the 10k
subsets in Table @tbl:avg_bench, and the 10k largest metagenomes. {#tbl:catalog}

![**branchwater scales well with number of threads.** Processing time drops linearly with number of threads, while total compute stays approximately the same and memory usage increases linearly with number of threads as each thread loads a subject to search.](images/basic_benchmarks.svg "basic benchmarking"){#fig:basic_bench}

<!-- CTB: should note that one thread is used exclusively for writing to CSV. -->

![**branchwater scales linearly with number of queries and subjects, but number of subjects dominates runtime.** Processing time increases slowly with number of query genomes used to search, because they are held in memory and fast to compare. Processing time increases quickly with number of subject metagenomes being searched, because they are large and slow to load and search.](images/subsample_benchmarks.svg "branchwater scaling"){#fig:basic_scaling}

See https://github.com/dib-lab/2022-branchwater-benchmarking for
number processing and figure generating notebooks.

### Post-search validation

The sourmash CLI can be used to explore k-mer matches for individual
data sets. This does not validate the matches beyond confirming the
containment numbers, although sourmash provides additional information
(e.g. estimated abundances) on top of the minimal information
provided by branchwater.  FracMinHash generally and Branchwater
specifically have been validated bioinformatically primarily by
mapping reads (see [@gather; @lumian_biogeo]). This is discussed
further below.

<!--
### branchwater is inexpensive and supports exploratory queries

CTB: Estimate cost of a run. Compare to serratus - cloud compute, data
download. Serratus is probably cheaper than $20k now but still
expensive.

-->
