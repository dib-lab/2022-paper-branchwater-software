## Methods {.page_break_before}

*The Methods should include a subsection on Implementation describing how the tool works and any relevant technical details required for implementation; and a subsection on Operation, which should include the minimal system requirements needed to run the software and an overview of the workflow.*

### Implementation of sra_search - rust; uses sourmash core.

It loads all the queries into memory and then uses threads to iterate across the provided collection of search sketches.

From luiz thesis: it is possible to leverage the core features to prototype a large scale search method in Rust that can efficiently keep queries in memory and process batches of the metagenome signatures without needing to store them in memory first, loading on demand and releasing resources after they are queried. This approach also benefits from queries being effectively immutable, and so can be easily shared without data races by multiple processing threads.

sra_search takes in search parameters as well as two text files, one containing a list of query file paths and one containing a lits of subject file paths. Upon execution, it reports the number of query sketches loaded and the number of subject file paths found, and then begins the search. It progressively reports the number of sketches searched in blocks of 10000 along with any matches.

### Performance - scales linearly in memory and time with queries and # threads


A variety of simple benchmarks will presumably show:
* Speed increases linearly with number of threads
* Memory scales linearly: sum(queries) + sum(threads\*average size of sketches)
* What happens w/biggest sketches?
* Complexity is n(query) \* n(subject), with subject loading being the dominant practical time. More complex indexing and query foo could be done but it’s fast enough and the code is simple.

Sra_search is largely I/O bound. Presumably we could speed it up a bit by distributing sketches to various nodes but in practice this is logistically challenging to coordinate. 13 TB is large. In a cloud environment with fast interconnect other design decisions could be made. But also other query systems could be built.

### Sra_search is inexpensive and supports exploratory queries

Estimate cost of a run. Compare to serratus - cloud compute, data download. Serratus is probably cheaper than $20k now but still expensive.

### Immediate validation etc.

It is straightforward to use sourmash CLI to query the metagenome data sets to double check magsearch results. This is usually only internal technical validation since magsearch is built on the same code that sourmash uses, but is a recommended first step because the sourmash UX is better and the output is richer (e.g. weighted abundances,e tc.)

FracMinHash generally and MAGsearch specifically have been validated in a more scientific sense primarily by mapping reads. This is addressed below.

### Design alternatives - (could be moved to discussion)

Sra search is a simple yet extremely effective initial implementation that supports a number of use cases. As its use increases many improvements are possible.

Roads not (yet) taken include:

Indexing the data sets in some way. Colors present a challenge. (Mastiff)
Organizing data sets in some way based on content. Clustering and data set organization present a challenge at this scale.
Putting a Bloom filter in front of each data set, and/or implementing SBT. Unbalanced data sets and additional storage present a challenge.
Revising on-disk format; progressive loading; etc.

Lack of auxiliary data structures is also a feature… allows us to update collection quickly.

### Practically running MAGsearch

Usually run in snakemake workflow, which sets RAYON threads etc.
Here are the repos.

### Following up on MAGsearch results - (move to Discussion?)

Many MAGsearch use cases are intended for early-stage hypothesis generation and refinement i.e. “hit to lead”, and hence MAGsearch is an early stage in conceptual and concrete workflows. Typical immediate concerns after receiving MAGsearch results are (1) what is the right threshold for my results (2) are my results at that threshold valid (3) how do I get my hands on the actual data, not just the sketches.

The first analysis step taken is often picking a threshold. The exact approach taken will vary depending on use case, but many use cases are looking for or expecting specific distributions of ScientificName so we have provided a simple script that imports SRA metadata and summarizes the MAGsearch results at that threshold. (example output)

After that, many paths can be taken.

Most metagenome data sets are Illumina short-read sequencing, and a plethora of general purpose bioinformatics tools exist for mapping and assembling.

Two tools that were developed in concert with sourmash and MAGsearch are genome-grist and spacegraphcats.

Genome-grist performs an entirely automated reference-based characterization of individual metagenomes that combines sourmash gather / minimum metagenome cover with mapping; it is described in Irber et al and was used in Lumian et al. Given that it does download all the data and maps all the reads, it is still relatively lightweight.

spacegraphcats is an assembly-graph based investigative tool for metagenomes that retrieves graph neighborhoods from metagenome assembly graphs for the purpose of investigating strain variation. It was used in Reiter et al., and Lumian et al. (phormidium paper). It is much heavier weight than genome-grist because it uses assembly graphs.

