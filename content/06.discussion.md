## Discussion {.page_break_before}

Enabling content-based search of very large collections of sequencing data
is an open problem, and approaches that work for smaller collections
rarely scale well, even for current database sizes. New methods that
take advantage of specific particularities of the query and desired
answer can help bridge the gap between more general methods by
allowing filtering of large databases, resulting in more manageable
subsets that can be used efficiently with current in depth analysis methods.

K-mer search via sourmash branchwater is a lightweight, scalable
approach to content-based search of petabase-scale collections of
sequencing data. Branchwater is designed to recover data sets
with high nucleotide similarity to the query sequence (>= 90% ANI)
across the Sequence Read Archive using computational resources
readily available to many researchers. As this approach enables content-based
search across petabases of sequence data, we anticipate it will be
most useful for filtering large databases to generate a manageable
subset of relevant data sets that can be analyzed in detail using other
tools.

Lumian et al. (2022) validated branchwater results by downloading
matching Illumina metagenomes above a specific containment threshold
and mapping the reads back to the query genomes to evaluate both
mapping detection and effective coverage. In all but one case,
k-mer-based genome detection of the query was lower than mapping-based
detection - in some cases significantly so. This was also seen with a
a smaller set of samples in Irber et al., 2022 [@gather], and is
likely because mapping-based approaches can tolerate mismatches, while
k-mer based approaches require exact mismatches.

While demonstrably useful, branchwater search has limitations, some of
which are intrinsic to the approach, and some of which can be overcome
with improved database storage and search design.  First, branchwater
search relies upon exact matching of long nucleotide k-mers, which
work best for sequences with high sequence identity (90%+ ANI),
particularly when combined with thresholding used for fast search.
Approaches using alignment-based techniques may be more useful for
detecting similarity across larger evolutionary distances [@serratus].

Second, as designed, branchwater cannot robustly detect sequence
similarity for data sets under 10kb in size. This limit is related to
the scaling approach used to reduce the effective search space and
enable petabase-scale search.  Branchwater leverages FracMinHash
sketching to build a compressed representation of each data set, which
enables direct and accurate sequence similarity and containment
comparisons without needing to access the original sequencing reads.
Because only a fraction of the original data needs to be stored,
FracMinHash sketches are good basic components in the implementation
of systems that allow searching large collections of datasets.
However, this approach comes with some detection limitations.  Robust
detection requires a minimum overlap of 2-3 hashes [@gather]. With the
$S=1000$ parameters used in branchwater, this represents approximately
2-3 kb of matching sequence.  Since many plasmids and most bacterial
and archaeal genomes are far larger than 10kb, branchwater is well
suited to detecting matches to such sequences in the SRA.

Here we note that branchwater has a one-sided error: matches of 3
hashes are reliable guarantees of the presence of sequence, while no
match does not guarantee absence of sequence. In particular, sequences
smaller than 10kb may be missed despite being present.  Future storage
and search optimizations may enable higher resolution search.

Interestingly, k-mer detection and FracMinHash specifically is quite
sensitive even when genome *coverage* is low. This is because (1) 1x
sequencing coverage of a genome will yield approximately 2/3 of the
k-mers in that genome, (2) 21-mers, 31-mers, and 51-mers are all
highly specific in combination, and (3) genomes are large, so
detecting 3 or more hashes indicates that 3kb or more overlap is
present.

<!-- CTB cite detection, metapalette, gather; revisit paragraph; graph? -->

### Tackling biological use cases with branchwater

K-mer search via branchwater has been used in two projects so far -
Lumian et al. (2022) [@lumian_biogeo] and Viehweger et al. (2021)
[@viehweger]. Viehweger et al. used branchwater to find a metagenome
containing an additional *Klebsiella pneumoniae* for a large scale
analysis of outbreak data, while Lumian et al. conducted a global
biogeography analysis of five new antarctic cyanobacteria. Both
studies benefited from the low cost and comprehensive nature of the
search.

We expect a broader range and more elaborate set of use cases to
emerge as petabase scale search becomes more widely available. The low
cost of search with branchwater is particularly enabling for
exploratory efforts, although the sheer size of the underlying data
needed even for branchwater continues to present obstacles.

There are several scientific limitations of our approach to overcome
as well. The current search approach has limited sensitivity to
divergent sequence beyond the genus level, and cannot find smaller
matches. These are topics for future research and development.

### Following up on branchwater results

Many branchwater use cases are intended for early-stage hypothesis
generation and refinement, i.e. branchwater implements the first part
of a “hit to lead” pipeline. Hence branchwater operates at an early
stage in both conceptual and concrete workflows. The next steps
after executing branchwater are (1) choosing a threshold
at which to filter results, (2) evaluating the overall results by type
of metagenome retrieved, and (3) retrieving the data underlying the
matches.

Branchwater search is exhaustive and can be too sensitive for many
applications: that is, branchwater may return too many results for
detailed investigation. Thus the first analysis step taken
is typically picking a more stringent threshold. The number of metagenomes
results is highly dependent on the query, ranging from dozens of results
for organisms from understudied environments to tens or hundreds of
thousands of results for organisms from well-sequenced environments,
such as human gut search in (@tbl:match_categories). As a result,
thresholds are typically chosen based on the use case and the observed
distribution of the annotated metagenome type (`ScientificName` from
the SRA Runinfo database). After filtering, many paths can be taken.
A plethora of general purpose bioinformatics tools exist for working
with the data from individual metagenomes.

We have built two custom tools in concert with sourmash and
branchwater, genome-grist and spacegraphcats.  Genome-grist performs
an entirely automated reference-based characterization of individual
metagenomes that uses the minimum metagenome cover produced by
sourmash gather to guide mapping of short reads; it is described in Irber
et al. [@gather] and was used in Lumian et al. [@lumian_biogeo]. While
genome-grist does download the entire data set in order to map the
reads, it is reference based and thus relatively lightweight.

spacegraphcats is an assembly-graph based investigative tool for
metagenomes that retrieves graph neighborhoods from metagenome
assembly graphs for the purpose of investigating strain variation
[@spacegraphcats]. It was used to retrieve putative accessory elements
from sourmash matches in Reiter et al. (2022) [@reiter_ibd] and Lumian
et al. (2021) [@lumian_phormidium]. It is much heavier weight than
genome-grist because it relies on a compact De Bruijn graph, which is
expensive to build for very rich or diverse metagenomes.

### Design alternatives to the current branchwater implementation

The current branchwater software is a simple and effective
implementation that is easy to analyze algorithmically and supports a
number of use cases. However, many improvements are possible:
FracMinHash analyses are based on comparing collections of 64-bit
integers, and there are many effective tools and approaches for
organizing and searching such collections more efficiently than is
presently done.

One area for particular improvement is storing and loading sketches
more efficiently. The current JSON-based format is convenient for
debugging and multi-language interoperability but is extremely
inefficient. Moreover, each file currently contains three k-mer
sketches (one per each desired k-mer search size), which means
approximately 3 times as much data is loaded per file than is actually
used in a comparison.  FracMinHash also could support fractional
loading, i.e. decreased resolution by loading only the bottom portion
of the sketch; this would enable must faster searches albeit at lower
resolution. This is not yet supported by the underlying sourmash
library.

<!-- XXX also talk about david's alternate sketch format -->

Currently the data files are organized as flat files in a single
directory on a single network file system. There are a variety of
practical ways to speed up the search by distributing sketch files
across multiple nodes, but this is logistically challenging. In
particular, our current usage involves running branchwater once every
few weeks on our HPC, which does not have sufficient local storage to
distribute the data sets across nodes. In addition, the speed savings from
distributing sketches across nodes is unlikely to be rewarding enough
to offset the maintenance requirements for a distributed collection of
7.5 TB of sketches.  Future work could include implementation of an
automated distribution system, although careful evaluation of the
maintenance and update requirements would be needed.

We could also create a simple pre-filter for each file using a data
structure with one-sided error. For example, we could create a Bloom
filter for each sketch that could be used to estimate containment
prior to loading the full sketch file. However, for some potentially
common use cases such as queries with many matches, this could add
significant I/O without speeding up the overall search.

Building an inverted index that maps hashes to data sets could also
enable rapid queries. Two challenges here are the scale of the catalog
and the number of data sets; the total number of hashes present in our
metagenome catalog is 375 billion hashes, across nearly 800,000 data sets.

Despite these many opportunities for optimization, we argue that there
is a significant benefit to the simplicity of our current approach. In
particular, providing the sketches in individual files organized by
accession makes it straightforward to access individual sketches by a
distinct ID and quickly update the overall metagenome catalog.  This
is particularly valuable since the sourmash Python package provides a
flexible suite of tools for inspecting and manipulating individual
metagenome sketches.  The lack of auxiliary data structures also
avoids expensive load and synchronization steps when adding new
datasets. These features are important for downstream user
investigation as well as maintainability and correctness, which are
important considerations in any scientific software workflow.

## Conclusion {.page_break_before}

We provide a flexible and fast petabase-scale search based on
FracMinHash, together with some simple downstream summarization tools
and an increasingly mature (albeit slower) investigative ecosystem.
This supports and enables a wide range of biological use cases that
take advantage of public data; these use cases range from biomedical
to ecological to technical (Table @tbl:bio-use-cases).

## Data availability statement:

All of the original data for the sketches in the branchwater database
is available from the NCBI Sequence Read Archive. A current catalog of
the SRA accessions is available at XXX. The sketch collection is 7.5
TB and is available upon request.  All sourmash sketches are provided
under Creative Commons Zero (CC0) - No Rights Reserved.

<!-- CTB add catalog -->
